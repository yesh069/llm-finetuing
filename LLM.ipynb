{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3Z1tj--1TEa",
        "outputId": "e98c48ef-7541-4f07-f080-31fdc8d13502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPJhDe3O4JYd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FikGRwFD4JWN",
        "outputId": "8d511bb0-9abb-46de-bd48-35f7c53431a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6cB-qIk4JT6"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRjnB6ZE4JRY",
        "outputId": "d6046004-1126-4bb8-c1bc-bd8d43aeb20e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "file_path = \"LLM-Sample-Input-File.csv\"\n",
        "train_data = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=file_path,\n",
        "    block_size=128\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msa-_Wvt4JOd",
        "outputId": "5bbe90b7-65b3-47be-a1b6-1f5c4e47aad9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([4, 128])\n",
            "torch.Size([1, 128])\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scaler = GradScaler()\n",
        "\n",
        "num_epochs = 1\n",
        "accumulation_steps = 8\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        print(inputs.shape)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        labels = inputs.clone()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs.loss / accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GezTTLII4JL1",
        "outputId": "10703652-e949-4227-bec4-ab8f71675a92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./my_finetuned_model/tokenizer_config.json',\n",
              " './my_finetuned_model/special_tokens_map.json',\n",
              " './my_finetuned_model/vocab.json',\n",
              " './my_finetuned_model/merges.txt',\n",
              " './my_finetuned_model/added_tokens.json')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model.save_pretrained(\"./my_finetuned_model\")\n",
        "tokenizer.save_pretrained(\"./my_finetuned_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU9hpJua1p1X"
      },
      "outputs": [],
      "source": [
        "model_path = \"./my_finetuned_model\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path, pad_token_id=tokenizer.eos_token_id)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWIuxvwZ45cS"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RDFasX06LpJ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"`do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE1fPJMQ45aA"
      },
      "outputs": [],
      "source": [
        "def get_model_response(user_input):\n",
        "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, attention_mask=attention_mask, max_length=100, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT2ToWOu45Xe",
        "outputId": "2422097d-0f38-4d79-c13d-797263a134af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Hi! I'm your chatbot. Type 'exit' to end the conversation.\n",
            "You: How much revenue does Potato Inc. make from selling Smartphones\n",
            "Chatbot: How much revenue does Potato Inc. make from selling Smartphones?\n",
            "\n",
            "Potato Inc.'s revenue is based on the number of Smartphone units sold in the U.S. during the first quarter of 2013.\n",
            "The company's revenue from the sale of smartphones is estimated to be between $1.5 million and $2 million per quarter, depending on how many units are sold and how long it takes for them to arrive in customers' hands. The company does not disclose how much it\n",
            "You: How much revenue does Potato Inc. make from Japan\n",
            "Chatbot: How much revenue does Potato Inc. make from Japan?\n",
            "\n",
            "Potato Inc.'s total revenue in Japan is estimated to be around $1.5 billion. This figure is based on the assumption that the company will be able to sell its potato products in the country for at least one year, and that it will generate enough revenue to cover its operating expenses for the next year. However, this estimate does not take into account the fact that there are many factors that affect the profitability of a company\n",
            "You: exit\n"
          ]
        }
      ],
      "source": [
        "print(\"Chatbot: Hi! I'm your chatbot. Type 'exit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    model_response = get_model_response(user_input)\n",
        "    print(\"Chatbot:\", model_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "felMS54n5VU7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
